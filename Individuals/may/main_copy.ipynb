{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import os\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "file1 = {\n",
    "    'application_test': 'application_test.csv',\n",
    "    'application_train': 'application_train.csv'\n",
    "    }\n",
    "\n",
    "file2 = {\n",
    "    'bureau_balance': 'bureau_balance.csv',\n",
    "    'bureau': 'bureau.csv',\n",
    "    'credit_card_balance': 'credit_card_balance.csv',\n",
    "    'installments_payments': 'installments_payments.csv',\n",
    "    'POS_CASH_balance': 'POS_CASH_balance.csv',\n",
    "    'previous_application': 'previous_application.csv'\n",
    "    }\n",
    "\n",
    "for key, name in file1.items():\n",
    "    data[key] = pd.read_csv(f'../dataset/{name}', index_col=0)\n",
    "    print(f'Dataset: {key} - Shape: {data[key].shape}')\n",
    "\n",
    "for key, name in file2.items():\n",
    "    data[key] = pd.read_csv(f'../dataset/{name}')\n",
    "    print(f'Dataset: {key} - Shape: {data[key].shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning + create new features + group values of each feature + encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(df, col_list):\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    one_hot_encoded = encoder.fit_transform(df[col_list])\n",
    "    one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(col_list))\n",
    "    df = pd.concat([df, one_hot_df], axis=1)\n",
    "    df = df.drop(col_list, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoding(df, col_list):\n",
    "    for col in col_list:\n",
    "        label_encoder = preprocessing.LabelEncoder() \n",
    "        df[col] = label_encoder.fit_transform(df[col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_outlier(df):\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checking_missing_values(df):\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Percentage of null in null columns\n",
    "def percentage_null(df):\n",
    "    missing_percent = df.isnull().sum()/df.shape[0]*100\n",
    "    return missing_percent[missing_percent > 0].sort_values(ascending=False)\n",
    "\n",
    "for key in data.keys():\n",
    "    print(f'Percentage of missing values in {key}:')\n",
    "    print(percentage_null(data[key]),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.application_train + application_tesst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def application_processing(df):\n",
    "    #CLEANING\n",
    "    df.loc[df['DAYS_EMPLOYED'] == 365243, 'DAYS_EMPLOYED'] = np.nan \n",
    "    df.loc[df['CODE_GENDER'] == 'XNA', 'CODE_GENDER'] = 'F'\n",
    "\n",
    "    df.loc[(df['NAME_FAMILY_STATUS'] == 'Unknown') | (df['NAME_FAMILY_STATUS'] == 'Civil marriage'), 'NAME_FAMILY_STATUS'] = 'Married'\n",
    "    df.loc[(df['NAME_FAMILY_STATUS'] == 'Single / not married') | (df['NAME_FAMILY_STATUS'] == 'Separated') | (df['NAME_FAMILY_STATUS'] == 'Widow'),\\\n",
    "            'NAME_FAMILY_STATUS'] = 'Unmarried'\n",
    "\n",
    "    df.loc[(df['NAME_TYPE_SUITE'] == 'Other_A') | (df['NAME_TYPE_SUITE'] == 'Other_B'), 'NAME_TYPE_SUITE'] = 'Other'\n",
    "    df['ORGANIZATION_TYPE'] = df['ORGANIZATION_TYPE'].str.replace(':', '').str.split(' ').str[0]\n",
    "\n",
    "    mapping_education_type = {\n",
    "        'Lower secondary': 1,\n",
    "        'Secondary / secondary special': 2,\n",
    "        'Incomplete higher': 3,\n",
    "        'Higher education': 4,\n",
    "        'Academic degree': 5\n",
    "    }\n",
    "    df['NAME_EDUCATION_TYPE'] = df['NAME_EDUCATION_TYPE'].apply(lambda x : mapping_education_type[x])\n",
    "    \n",
    "    #CREATE NEW COLUMNS\n",
    "    df['DAYS_EMPLOYED_PERCENT'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['INCOME_CREDIT_PERCENT'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "    df['INCOME_PER_MEMBER'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    df['ANNUITY_INCOME_PERCENT'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "\n",
    "    #FLAG_CONTACT(PHONE + EMAIL)\n",
    "    df['FLAG_CONTACT_PHONE'] = df['FLAG_CONT_MOBILE']*(df['FLAG_EMP_PHONE'] + df['FLAG_WORK_PHONE'])\n",
    "    df = df.drop(['FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE', 'FLAG_PHONE', 'FLAG_EMAIL'], axis=1)\n",
    "\n",
    "    #FLAG_DOCUMENT\n",
    "    df['FLAG_DOCUMENT_36'] = df['FLAG_DOCUMENT_3'] * (df['FLAG_DOCUMENT_6'] + df['FLAG_DOCUMENT_3'])\n",
    "    df = df.drop(['FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3', 'FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6', \n",
    "                'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', \n",
    "                'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', \n",
    "                'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21'\n",
    "                ], axis=1)\n",
    "    \n",
    "    #HOUSE\n",
    "    \n",
    "    #RENAME COLUMN FOR MERGING\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.bureau + bureau_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_bureau(data):\n",
    "    # Drop all the unbelievable values\n",
    "    data = data.loc[(data['DAYS_CREDIT_UPDATE'] >= -2922)]\n",
    "    data = data.loc[(data['DAYS_ENDDATE_FACT'] >= -2922) |\n",
    "                    (data['DAYS_ENDDATE_FACT'].isna())]\n",
    "    data = data.loc[(data['DAYS_CREDIT_ENDDATE'] >= -2922)]\n",
    "\n",
    "    # For loans that due in more than 50 years, we replace it with NaN\n",
    "    data.loc[(data['DAYS_CREDIT_ENDDATE'] > 50*365)] = np.nan\n",
    "\n",
    "    # Change CREDIT_ACTIVE to Closed for DAYS_ENDDATE_FACT < 0\n",
    "    data.loc[(data['DAYS_ENDDATE_FACT'] < 0) & \n",
    "             (data['CREDIT_ACTIVE'] == 'Active'), 'CREDIT_ACTIVE'] = 'Closed'\n",
    "    \n",
    "    # Drop column with high missing values and not useful\n",
    "    data = data.drop(['AMT_ANNUITY', 'CREDIT_CURRENCY'], axis=1)\n",
    "    \n",
    "    # Drop entries with AMT_CREDIT_SUM = 0\n",
    "    data = data.loc[data['AMT_CREDIT_SUM'] != 0]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_bureau_balance(bureau_balance):\n",
    "    # Change the negative value to positive\n",
    "    bureau_balance['MONTHS_BALANCE'] = np.abs(bureau_balance['MONTHS_BALANCE'])\n",
    "    \n",
    "    status_dict = {'C': 0, '0': 1, '1': 2, '2': 3, 'X': 4, '3': 5, '4': 6, '5': 7}\n",
    "    bureau_balance['STATUS'] = bureau_balance['STATUS'].map(status_dict)\n",
    "\n",
    "    bureau_balance['WEIGHTED_STATUS'] = bureau_balance['STATUS'] / (bureau_balance['MONTHS_BALANCE'] + 1) # Avoid division by zero\n",
    "    bureau_balance = bureau_balance.sort_values(\n",
    "        by=['SK_ID_BUREAU', 'MONTHS_BALANCE'], ascending=[True, True]\n",
    "        ) # Sorting to calculate moving averages\n",
    "\n",
    "    bureau_balance['MONTHS_BALANCE'] = bureau_balance['MONTHS_BALANCE'] // 12\n",
    "\n",
    "    # Using exponential weighted moving average to calculate the weighted status\n",
    "    bureau_balance['EXP_WEIGHTED_STATUS'] = bureau_balance.groupby('SK_ID_BUREAU')['WEIGHTED_STATUS'] \\\n",
    "                                                        .transform(lambda x: x.ewm(alpha = 0.7).mean())\n",
    "    \n",
    "    # Using exponential moving average to calculate the status\n",
    "    bureau_balance['EXP_ENCODED_STATUS'] = bureau_balance.groupby('SK_ID_BUREAU')['STATUS'] \\\n",
    "                                                        .transform(lambda x: x.ewm(alpha = 0.7).mean())\n",
    "    \n",
    "\n",
    "    # Aggregating data for each SK_ID_BUREAU\n",
    "    bureau_balance_agg = bureau_balance.groupby(['SK_ID_BUREAU']).agg({\n",
    "        'MONTHS_BALANCE' : ['mean','max'],\n",
    "        'STATUS' : ['mean','max','first'],\n",
    "        'WEIGHTED_STATUS' : ['mean','sum','first'],\n",
    "        'EXP_WEIGHTED_STATUS' : ['last'],\n",
    "        'EXP_ENCODED_STATUS' : ['last']\n",
    "        })\n",
    "    bureau_balance_agg.columns = ['_'.join(ele).upper() for ele in bureau_balance_agg.columns]\n",
    "\n",
    "    # Aggregating data for the last 3 years\n",
    "    balance_agg_all_years = pd.DataFrame()\n",
    "    for period in range(3):\n",
    "        period_group = bureau_balance.loc[bureau_balance['MONTHS_BALANCE'] == period].groupby('SK_ID_BUREAU').agg({\n",
    "            'STATUS': ['mean', 'max', 'last', 'first'],\n",
    "            'WEIGHTED_STATUS': ['mean', 'max', 'first', 'last'],\n",
    "            'EXP_WEIGHTED_STATUS': ['last'],\n",
    "            'EXP_ENCODED_STATUS': ['last']\n",
    "        })\n",
    "        \n",
    "        period_group.columns = ['_'.join(col).upper() + '_PERIOD_' + str(period) for col in period_group.columns]\n",
    "\n",
    "        if period == 0:\n",
    "            balance_agg_all_years = period_group\n",
    "        else:\n",
    "            balance_agg_all_years = balance_agg_all_years.merge(period_group, on='SK_ID_BUREAU', how='outer')\n",
    "\n",
    "\n",
    "    balance_agg_rest_years = bureau_balance[bureau_balance.MONTHS_BALANCE > period] \\\n",
    "                    .groupby(['SK_ID_BUREAU']).agg({\n",
    "                                                'STATUS' : ['mean','max','last','first'],\n",
    "                                                'WEIGHTED_STATUS' : ['mean','max', 'first','last'],\n",
    "                                                'EXP_WEIGHTED_STATUS' : ['last'],\n",
    "                                                'EXP_ENCODED_STATUS' : ['last'] \n",
    "                                                })\n",
    "    balance_agg_rest_years.columns = ['_'.join(ele).upper() + '_THE_REST' for ele in balance_agg_rest_years.columns]\n",
    "\n",
    "    balance_agg_all_years = balance_agg_all_years.merge(balance_agg_rest_years, on = 'SK_ID_BUREAU', how = 'outer')\n",
    "    bureau_balance_agg = bureau_balance_agg.merge(balance_agg_all_years, on = 'SK_ID_BUREAU', how = 'inner')\n",
    "\n",
    "    bureau_balance_agg.fillna(0, inplace = True)\n",
    "    \n",
    "    return bureau_balance_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_bureau(bureau, final_bureau_balance):\n",
    "    # Merge the bureau_balance with bureau\n",
    "    bureau = bureau.merge(final_bureau_balance, on='SK_ID_BUREAU', how='left')\n",
    "\n",
    "    # New features for DAYS columns:\n",
    "    bureau['CREDIT_AGE'] = abs(bureau['DAYS_CREDIT'])\n",
    "    bureau['CREDIT_DURATION'] = abs(bureau['DAYS_CREDIT'] - bureau['DAYS_CREDIT_ENDDATE'])\n",
    "    bureau['DAYS_CREDIT_LEFT'] = bureau['DAYS_CREDIT_ENDDATE'] - bureau['DAYS_CREDIT']\n",
    "    bureau['FLAG_IS_OVERDUE'] = bureau['CREDIT_DAY_OVERDUE'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    bureau['DAYS_OVERDUE_DURATION'] = bureau['CREDIT_DAY_OVERDUE'].apply(lambda x: max(0,x))\n",
    "    bureau['DAYS_OVERDUE_RATIO'] = bureau['CREDIT_DAY_OVERDUE'] / (bureau['CREDIT_DURATION'] + 0.0001)\n",
    "    bureau['DAYS_EARLY_REPAYMENT'] = abs(bureau['DAYS_ENDDATE_FACT'] - bureau['DAYS_CREDIT_ENDDATE'])\n",
    "    bureau['FLAG_IS_EARLY_REPAYMENT'] = bureau['DAYS_CREDIT_ENDDATE'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    bureau['DAYS_SINCE_LAST_UPDATE'] = bureau['DAYS_CREDIT_UPDATE'] - bureau['DAYS_CREDIT']\n",
    "    bureau['CREDIT_ENDDATE_UPDATE_DIFF'] = abs(bureau['DAYS_CREDIT_UPDATE'] - bureau['DAYS_CREDIT_ENDDATE']) \n",
    "    \n",
    "    # New features for AMT columns:\n",
    "    bureau['CREDIT_UTILIZATION_RATIO'] = bureau['AMT_CREDIT_SUM_DEBT'] / (bureau['AMT_CREDIT_SUM'] + 0.0001)\n",
    "    bureau['AMT_OVERDUE_RATIO'] = bureau['AMT_CREDIT_SUM_OVERDUE'] / (bureau['AMT_CREDIT_SUM'] + 0.0001)\n",
    "    bureau['PROLONGATION_FREQUENCY'] = bureau['CNT_CREDIT_PROLONG'] / (bureau['CREDIT_DURATION'] + 0.0001)\n",
    "    bureau['CREDIT_DEBT_DIFF'] = bureau['AMT_CREDIT_SUM'] - bureau['AMT_CREDIT_SUM_DEBT']\n",
    "    bureau['DEBT_LIMIT_RATIO'] = bureau['AMT_CREDIT_SUM_DEBT'] / (bureau['AMT_CREDIT_SUM_LIMIT'] + 0.0001)\n",
    "    bureau['MAX_OVERDUE_DEBT_RATIO'] = bureau['AMT_CREDIT_MAX_OVERDUE'] / (bureau['AMT_CREDIT_SUM_DEBT'] + 0.0001)\n",
    "    bureau['TOTAL_RISK_SCORE'] = bureau['CREDIT_UTILIZATION_RATIO'] + bureau['AMT_OVERDUE_RATIO'] + bureau['DEBT_LIMIT_RATIO']\n",
    "    bureau['DEBT_CREDIT_RATIO'] = bureau['AMT_CREDIT_SUM_DEBT'] / (bureau['AMT_CREDIT_SUM_OVERDUE'] + 0.0001)\n",
    "    bureau['OVERDUE_SEVERITY'] = bureau['AMT_CREDIT_MAX_OVERDUE'] / (bureau['AMT_CREDIT_SUM_OVERDUE'] + 0.0001)\n",
    "    bureau['OVERDUE_DURATION_RATIO'] = bureau['DAYS_OVERDUE_DURATION'] / (bureau['CREDIT_DURATION'] + 0.0001)\n",
    "    bureau['OVERDUE_SEVERITY_RATIO'] = bureau['OVERDUE_SEVERITY'] / (bureau['DAYS_OVERDUE_DURATION'] + 0.0001)\n",
    "    bureau['RISK_EXPOSURE_RATIO'] = (bureau['AMT_CREDIT_SUM_DEBT'] + bureau['AMT_CREDIT_SUM_OVERDUE']) / (bureau['AMT_CREDIT_SUM_LIMIT'] + 0.0001)\n",
    "\n",
    "    # Combine all other credit type into 'Other' category (expect Consumer Credit, credit card, car loan, mortgage, microloan)\n",
    "    column_to_keep = ['Consumer credit', 'Credit card', 'Car loan', 'Mortgage', 'Microloan']\n",
    "    bureau['CREDIT_TYPE'] = bureau['CREDIT_TYPE'].apply(lambda x: x if x in column_to_keep else 'Other')\n",
    "\n",
    "    # Only keep Active and Closed status, change Sold and Bad Debt to Other\n",
    "    column_to_keep = ['Active', 'Closed']\n",
    "    bureau['CREDIT_ACTIVE'] = bureau['CREDIT_ACTIVE'].apply(lambda x: x if x in column_to_keep else 'Other')\n",
    "\n",
    "    # Aggregate with respect to 'SK_ID_CURR' in order to merge with application_train\n",
    "    # First, aggreagate based on the category of CREDIT_ACTIVE\n",
    "    aggregate_CREDIT_ACTIVE = {\n",
    "        'CREDIT_AGE' : ['mean', 'max', 'min'],\n",
    "        'CREDIT_DURATION' : ['mean', 'max', 'min'],\n",
    "        'DAYS_CREDIT_LEFT' : ['mean', 'max', 'min'],\n",
    "        'FLAG_IS_OVERDUE' : ['sum'],\n",
    "        'DAYS_OVERDUE_DURATION' : ['mean', 'max', 'min'],\n",
    "        'DAYS_OVERDUE_RATIO' : ['mean', 'max', 'min'],\n",
    "        'DAYS_EARLY_REPAYMENT' : ['mean', 'max'],\n",
    "        'FLAG_IS_EARLY_REPAYMENT' : ['sum'],\n",
    "        'DAYS_SINCE_LAST_UPDATE' : ['mean', 'max', 'min'],\n",
    "        'CREDIT_ENDDATE_UPDATE_DIFF' : ['mean', 'max', 'min'],\n",
    "        'CREDIT_UTILIZATION_RATIO' : ['mean', 'max', 'min'],\n",
    "        'AMT_OVERDUE_RATIO' : ['mean', 'max', 'min'],\n",
    "        'PROLONGATION_FREQUENCY' : ['mean', 'max', 'min'],\n",
    "        'CREDIT_DEBT_DIFF' : ['mean', 'max', 'min'],\n",
    "        'DEBT_LIMIT_RATIO' : ['mean', 'max', 'min'],\n",
    "        'MAX_OVERDUE_DEBT_RATIO' : ['mean', 'max', 'min'],\n",
    "        'TOTAL_RISK_SCORE' : ['mean', 'max', 'min'],\n",
    "        'DEBT_CREDIT_RATIO' : ['mean', 'max', 'min'],\n",
    "        'OVERDUE_SEVERITY' : ['mean', 'max', 'min'],\n",
    "        'OVERDUE_DURATION_RATIO' : ['mean', 'max', 'min'],\n",
    "        'OVERDUE_SEVERITY_RATIO' : ['mean', 'max', 'min'],\n",
    "        'RISK_EXPOSURE_RATIO' : ['mean', 'max', 'min'],\n",
    "        'DAYS_CREDIT' : ['mean','min','max','last'],\n",
    "        'CREDIT_DAY_OVERDUE' : ['mean','max'],\n",
    "        'DAYS_CREDIT_ENDDATE' : ['mean','max'],\n",
    "        'DAYS_ENDDATE_FACT' : ['mean','min'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['max','sum'],\n",
    "        'CNT_CREDIT_PROLONG': ['max','sum'],\n",
    "        'AMT_CREDIT_SUM' : ['sum','max'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['sum'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['max','sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['max','sum'],\n",
    "        'DAYS_CREDIT_UPDATE' : ['mean','min'],\n",
    "    }\n",
    "\n",
    "    agg_bureau_credit = pd.DataFrame()\n",
    "    agg_active_type = ['Active', 'Closed', 'Other']\n",
    "    for i, type in enumerate(agg_active_type):\n",
    "        bureau_type = bureau.loc[bureau['CREDIT_ACTIVE'] == type]\n",
    "        bureau_type_agg = bureau_type.groupby('SK_ID_CURR').agg(aggregate_CREDIT_ACTIVE)\n",
    "        bureau_type_agg.columns = pd.Index(['_'.join(col_name).upper() + '_CREDIT_ACTIVE_' + type.upper() \n",
    "                                            for col_name in bureau_type_agg.columns.tolist()])\n",
    "        \n",
    "        if i == 0:\n",
    "            agg_bureau_credit = bureau_type_agg\n",
    "        else:\n",
    "            agg_bureau_credit = agg_bureau_credit.merge(bureau_type_agg, on='SK_ID_CURR', how='outer')\n",
    "\n",
    "    # One-hot encoding for CREDIT_ACTIVE, CREDIT_TYPE\n",
    "    credit_active_dummies = pd.get_dummies(bureau['CREDIT_ACTIVE'], prefix='CREDIT_ACTIVE')\n",
    "    credit_type_dummies = pd.get_dummies(bureau['CREDIT_TYPE'], prefix='CREDIT_TYPE')\n",
    "    bureau = pd.concat([bureau, credit_active_dummies, credit_type_dummies], axis=1)\n",
    "    bureau = bureau.drop(['CREDIT_ACTIVE', 'CREDIT_TYPE'], axis=1)\n",
    "\n",
    "    # Finally, aggregate based on 'SK_ID_CURR'\n",
    "    bureau_agg = bureau.drop('SK_ID_BUREAU', axis = 1).groupby('SK_ID_CURR').agg('mean')\n",
    "    bureau_agg.columns = [col + '_MEAN_OVERALL' for col in bureau_agg.columns]\n",
    "    bureau_agg = bureau_agg.merge(agg_bureau_credit, on='SK_ID_CURR', how='outer')\n",
    "\n",
    "    return bureau_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bureau_processing(bureau, bureau_balance):\n",
    "    bureau = cleaning_bureau(bureau)\n",
    "    bureau_balance = feature_engineering_bureau_balance(bureau_balance)\n",
    "    bureau = feature_engineering_bureau(bureau, bureau_balance)\n",
    "\n",
    "    bureau = bureau.loc[:, bureau.isna().mean() < 0.5]\n",
    "    return bureau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.credit_card_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d.installments_payment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e.previous_application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_application_processing(df):\n",
    "    df.loc[df['DAYS_FIRST_DRAWING'] == 365243, 'DAYS_FIRST_DRAWING'] = np.nan \n",
    "    df.loc[df['DAYS_FIRST_DUE'] == 365243, 'DAYS_FIRST_DUE'] = np.nan \n",
    "    df.loc[df['DAYS_LAST_DUE_1ST_VERSION'] == 365243, 'DAYS_LAST_DUE_1ST_VERSION'] = np.nan \n",
    "    df.loc[df['DAYS_LAST_DUE'] == 365243, 'DAYS_LAST_DUE'] = np.nan \n",
    "    df.loc[df['DAYS_TERMINATION'] == 365243, 'DAYS_TERMINATION'] = np.nan \n",
    "    df.loc[df['SELLERPLACE_AREA'] == 4000000] = np.nan\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f.POS_CASH_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g.main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean dataset\n",
    "data['application_train'] = application_processing(data['application_train'])\n",
    "data['application_test'] = application_processing(data['application_test'])\n",
    "data['bureau_final'] = bureau_processing(data['bureau'], data['bureau_balance'])\n",
    "\n",
    "\n",
    "#Vì group theo SK_ID_CURR nên không sửa trực tiếp trên dataset chính\n",
    "data['previous_application_cleaned'] = previous_application_processing(data['previous_application'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature when merge\n",
    "#         train_data = previous_application[['AMT_CREDIT', 'AMT_ANNUITY', 'CNT_PAYMENT']].dropna()\n",
    "#         train_data['CREDIT_ANNUITY_RATIO'] = train_data['AMT_CREDIT'] / (train_data['AMT_ANNUITY'] + 1)\n",
    "#         #value to predict is our CNT_PAYMENT\n",
    "#         train_value = train_data.pop('CNT_PAYMENT')\n",
    "        \n",
    "#         #test data would be our application_train data\n",
    "#         test_data = data_to_predict[['AMT_CREDIT','AMT_ANNUITY']].fillna(0)\n",
    "#         test_data['CREDIT_ANNUITY_RATIO'] = test_data['AMT_CREDIT'] / (test_data['AMT_ANNUITY'] + 1)\n",
    "        \n",
    "#         lgbmr = LGBMRegressor(max_depth = 9, n_estimators = 5000, n_jobs = -1, learning_rate = 0.3, \n",
    "#                               random_state = 125)\n",
    "#         lgbmr.fit(train_data, train_value)\n",
    "#         #dumping the model to pickle file\n",
    "#         with open('cnt_payment_predictor_lgbmr.pkl', 'wb') as f:\n",
    "#             pickle.dump(lgbmr, f)\n",
    "#         #predicting the CNT_PAYMENT for test_data\n",
    "#         cnt_payment = lgbmr.predict(test_data)\n",
    "\n",
    "#Create new column PREV_COUNT which is the number of previous_application of each SK_ID_CURR to see correlation\n",
    "# appli_prev_app2 = data['application_train'][['SK_ID_CURR', 'TARGET']]\n",
    "# appli_prev_app2 = appli_prev_app2.merge(target_dist[['SK_ID_CURR', 'count']], on=['SK_ID_CURR'], how='left')\n",
    "# appli_prev_app2.rename(columns={'count': 'PREV_COUNT'}, inplace=True)\n",
    "# appli_prev_app2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection (check correlation) + Feature Engingeering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Build model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
